{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsp2znaQUAAc",
        "outputId": "b76e1401-2a51-41aa-c52e-21bd86318213"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "file_name = \"Dataset.csv\"\n",
        "test_size = 0.4\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/Mestrado/Projeto/LSTMLevel/data\"\n",
        "model_dir = \"/content/drive/MyDrive/Mestrado/Projeto/LSTMLevel/model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "423ZFbpMUKzT"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "\n",
        "def load_data(file_name):\n",
        "    data = pd.read_csv(Path(data_dir, file_name))\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_data(df, file_name):\n",
        "    df.astype(float).to_csv(Path(data_dir, file_name), index=False)\n",
        "    return None\n",
        "\n",
        "def adicionar_coluna_trip(df):\n",
        "    label_column = []\n",
        "    num_anterior = df['Valvula.VALVE_SHUTOFF.L'][0]\n",
        "\n",
        "    for valor in df['Valvula.VALVE_SHUTOFF.L']:\n",
        "        if valor == 0 and num_anterior == 1:\n",
        "            label_column.append(1)\n",
        "        else:\n",
        "            label_column.append(0)\n",
        "\n",
        "        num_anterior = valor\n",
        "\n",
        "    df['Trip'] = label_column\n",
        "\n",
        "    return df\n",
        "\n",
        "def adicionar_coluna_label(df):\n",
        "    df['Label'] = df['Drum.V1.L']\n",
        "\n",
        "    return df\n",
        "\n",
        "def change_datetime(df):\n",
        "    data_atual = datetime.now()\n",
        "    segundos = [timedelta(seconds=i) for i in range(len(df))]\n",
        "    df['Datetime'] = data_atual + pd.to_timedelta(segundos)\n",
        "\n",
        "    return df\n",
        "\n",
        "def arredondar_dataframe(df, casas_decimais=3):\n",
        "    return df.round(casas_decimais)\n",
        "\n",
        "def create_features(df):\n",
        "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "\n",
        "    df = adicionar_coluna_trip(df)\n",
        "    df = adicionar_coluna_label(df)\n",
        "\n",
        "    df = df.drop(columns=['Datetime'])\n",
        "\n",
        "    contagem = (df['Trip'] == 1).sum()\n",
        "    print('{} eventos de trip'.format(contagem))\n",
        "    print('{} Max'.format(contagem.max()))\n",
        "\n",
        "    return df\n",
        "\n",
        "def rescale_data(df):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler = scaler.fit(df)\n",
        "\n",
        "    df_scaled = pd.DataFrame(\n",
        "        scaler.transform(df),\n",
        "        index=df.index,\n",
        "        columns=df.columns)\n",
        "\n",
        "    joblib.dump(scaler, Path(model_dir, 'scaler_level.gz'))\n",
        "\n",
        "    return df_scaled\n",
        "\n",
        "def split_data(df, test_size):\n",
        "    train, test = train_test_split(df, test_size=test_size, shuffle=False)\n",
        "    return train, test\n",
        "\n",
        "def remove_features(df):\n",
        "    columns_to_remove = [\n",
        "        'Variaveis.TR1_RUIDO',\n",
        "        'Variaveis.TR2_RUIDO',\n",
        "        'Variaveis.TR3_RUIDO',\n",
        "        'Transmissor.TR1.OUT',\n",
        "        'Transmissor.TR2.OUT',\n",
        "        'Transmissor.TR3.OUT',\n",
        "        'Trip'\n",
        "    ]\n",
        "\n",
        "    return df.drop(columns=columns_to_remove)\n",
        "\n",
        "def aplicar_ruidos(dataset):\n",
        "    novo_dataset = dataset.copy()\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        if np.random.rand() < 0.5:\n",
        "            colunas_ruido = np.random.choice(['Variaveis.TR1_RUIDO', 'Variaveis.TR2_RUIDO', 'Variaveis.TR3_RUIDO'], size=2, replace=False)\n",
        "            novo_dataset.loc[i, colunas_ruido] = True\n",
        "            novo_dataset.at[i, 'Variaveis.VOTACAO_TRANSMISSORES'] = novo_dataset.at[i, 'Transmissor.TR1.OUT']\n",
        "\n",
        "            for coluna in colunas_ruido:\n",
        "                novo_dataset.at[i, coluna] += np.random.uniform(0.01, 0.2)\n",
        "\n",
        "        elif np.random.rand() < 0.5:\n",
        "                coluna_w_s6 = 'Tubo.S6.W'\n",
        "                quantidade_registros_alterar = np.random.randint(1, 11)\n",
        "                valor_somar = np.random.randint(0, 2001)\n",
        "                novo_dataset.loc[i:i + quantidade_registros_alterar, coluna_w_s6] += valor_somar\n",
        "\n",
        "    return novo_dataset\n",
        "\n",
        "\n",
        "def prep_data(df, test_size, plot_df=False):\n",
        "    print(\"Starting with data preparation...\")\n",
        "\n",
        "    df = aplicar_ruidos(df)\n",
        "\n",
        "    df = change_datetime(df)\n",
        "    df = create_features(df)\n",
        "    df = remove_features(df)\n",
        "    df = arredondar_dataframe(df)\n",
        "\n",
        "    train_df, test_df = split_data(df, test_size)\n",
        "\n",
        "    if plot_df:\n",
        "        save_data(train_df, 'plot_df.csv')\n",
        "\n",
        "    train_df = rescale_data(train_df)\n",
        "    scaler = joblib.load(Path(model_dir, 'scaler_level.gz'))\n",
        "\n",
        "    test_df = pd.DataFrame(\n",
        "        scaler.transform(test_df),\n",
        "        index=test_df.index,\n",
        "        columns=test_df.columns)\n",
        "\n",
        "    save_data(train_df, 'train.csv')\n",
        "    save_data(test_df, 'test.csv')\n",
        "\n",
        "    print(\"Completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn84xkeSUmFc",
        "outputId": "3c0a127d-b6ac-441e-d51c-df5ed8dc0fee"
      },
      "outputs": [],
      "source": [
        "df = load_data(file_name)\n",
        "print(df.shape)\n",
        "prep_data(df, test_size, validation_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsBDqgKRjMoK"
      },
      "outputs": [],
      "source": [
        "#Lstm Multivariate Multi-Step\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from pandas import DataFrame , concat\n",
        "from sklearn.metrics import mean_absolute_error , mean_squared_error\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "from numpy import mean , concatenate\n",
        "from math import sqrt\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Activation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.layers import LSTM\n",
        "\n",
        "from numpy import array , hstack\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnQEQ6AUUI-c",
        "outputId": "1d1c3755-4d77-44ea-88dd-0d44f07ac5c7"
      },
      "outputs": [],
      "source": [
        "train_df = load_data('train.csv')\n",
        "test_df = load_data('test.csv')\n",
        "\n",
        "print(train_df.columns)\n",
        "print(test_df.columns)\n",
        "\n",
        "print(train_df.isna().sum().sum())\n",
        "print(test_df.isna().sum().sum())\n",
        "\n",
        "colunas_label = ['Label']\n",
        "\n",
        "X_train, y_train = np.array(train_df.loc[:, ~train_df.columns.isin(colunas_label)]), np.array(train_df[colunas_label])\n",
        "X_test, y_test = np.array(test_df.loc[:, ~test_df.columns.isin(colunas_label)]), np.array(test_df[colunas_label])\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZkzl0efURsq",
        "outputId": "95dbf3c0-ba57-4d77-a04e-c628e3e01d69"
      },
      "outputs": [],
      "source": [
        "def split_sequences(df_X, df_y, n_steps_in, n_steps_out):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(df_X)):\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out-1\n",
        "        if out_end_ix > len(df_X):\n",
        "            break\n",
        "        seq_x, seq_y = df_X[i:end_ix], df_y[end_ix-1:out_end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return array(X), array(y)\n",
        "\n",
        "n_steps_in, n_steps_out = 30, 15\n",
        "\n",
        "X_train_sequence, y_train_sequence = split_sequences(X_train, y_train, n_steps_in, n_steps_out)\n",
        "X_test_sequence, y_test_sequence = split_sequences(X_test, y_test, n_steps_in, n_steps_out)\n",
        "print (\"X_train_sequence.shape\" , X_train_sequence.shape)\n",
        "print (\"y_train_sequence.shape\" , y_train_sequence.shape)\n",
        "print (\"X_test_sequence.shape\" , X_test_sequence.shape)\n",
        "print (\"y_test_sequence.shape\" , y_test_sequence.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RPksErocdUI"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout, Flatten\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsoluteError\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def get_model_2(params, input_shape):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Bidirectional(LSTM(units=64, return_sequences=True, input_shape=(n_steps_in, input_shape))))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(16))\n",
        "\tmodel.add(Dense(n_steps_out))\n",
        "\n",
        "\tmodel.compile(loss=params[\"loss\"],\n",
        "              \toptimizer=params[\"optimizer\"],\n",
        "              \tmetrics=[RootMeanSquaredError(), MeanAbsoluteError()])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s63I7NfockQ5"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "\t\"loss\": \"mean_squared_error\",\n",
        "\t\"optimizer\": \"adam\",\n",
        "\t\"dropout\": 0.2,\n",
        "\t\"lstm_units\": 64,\n",
        "\t\"epochs\": 300,\n",
        "\t\"batch_size\": 128,\n",
        "\t\"es_patience\" : 10\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM_CVkVJHSaH",
        "outputId": "cd9b54a7-f0f9-4322-fd2b-bfd5a542583c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Versão do TensorFlow:\", tf.__version__)\n",
        "\n",
        "# Verifique a presença de uma GPU\n",
        "if tf.test.gpu_device_name():\n",
        "    print(\"GPU disponível:\", tf.test.gpu_device_name())\n",
        "else:\n",
        "    print(\"GPU não encontrada. Certifique-se de que está configurado corretamente.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg-ZYif4cms4",
        "outputId": "67b586f3-fbaa-4113-853f-e171b765ea2f"
      },
      "outputs": [],
      "source": [
        "model = get_model_2(params=params, input_shape=X_train_sequence.shape[2])\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits = 6)\n",
        "\n",
        "filepath = 'lstmlevel.epoch{epoch:02d}-loss{val_root_mean_squared_error:.5f}.hdf5'\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=filepath,\n",
        "\tverbose=1,\n",
        "    monitor='val_root_mean_squared_error',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error',\n",
        "                                           \tmode='min',\n",
        "                                            patience=params[\"es_patience\"])\n",
        "\n",
        "\n",
        "for train_index, test_index in tscv.split(X_train_sequence):\n",
        "\tX_train_split, X_val_split = X_train_sequence[train_index], X_train_sequence[test_index]\n",
        "\ty_train_split, y_val_split = y_train_sequence[train_index], y_train_sequence[test_index]\n",
        "\n",
        "\tmodel.fit(\n",
        "\t\tX_train_split,\n",
        "\t\ty_train_split,\n",
        "\t\tvalidation_data=(X_val_split, y_val_split),\n",
        "\t\tepochs=params[\"epochs\"],\n",
        "\t\tbatch_size=params[\"batch_size\"],\n",
        "\t\tverbose=1,\n",
        "\t\tcallbacks=[model_checkpoint_callback]\n",
        "\t\t#callbacks=[es_callback,model_checkpoint_callback]\n",
        "\t)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "4dtmLAXDh7Xa"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
